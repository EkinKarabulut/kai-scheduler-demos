apiVersion: v1
kind: Pod
metadata:
  name: training-job-1
  labels:
    runai/queue: project-a
spec:
  schedulerName: kai-scheduler
  containers:
  - name: training
    image: gcr.io/run-ai-demo/quickstart-demo
    resources:
      requests:
        nvidia.com/gpu: "2"
      limits:
        nvidia.com/gpu: "2"
---
apiVersion: v1
kind: Pod
metadata:
  name: training-job-2
  labels:
    runai/queue: project-c
spec:
  schedulerName: kai-scheduler
  containers:
  - name: training
    image: gcr.io/run-ai-demo/quickstart-demo
    resources:
      requests:
        nvidia.com/gpu: "1"
      limits:
        nvidia.com/gpu: "1"
---
apiVersion: "kubeflow.org/v1"
kind: "PyTorchJob"
metadata:
  name: "pytorch-distributed-mnist-nccl-training"
  labels:
    runai/queue: project-b
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          schedulerName: kai-scheduler
          containers:
            - name: pytorch
              image: ghcr.io/kubeflow/training-v1/pytorch-dist-mnist:latest
              args: ["--backend", "nccl", "--batch-size", "2048", "--epochs", "900"]
              # Comment out the below resources to use the CPU.
              resources:
                limits:
                  nvidia.com/gpu: "1"
    Worker:
      replicas: 2
      restartPolicy: OnFailure
      template:
        spec:
          schedulerName: kai-scheduler
          containers:
            - name: pytorch
              image: ghcr.io/kubeflow/training-v1/pytorch-dist-mnist:latest
              args: ["--backend", "nccl", "--batch-size", "2048", "--epochs", "900"]
              # Comment out the below resources to use the CPU.
              resources:
                limits:
                  nvidia.com/gpu: "1"
---
apiVersion: v1
kind: Pod
metadata:
  name: interactive-job
  labels:
    runai/queue: project-d
spec:
  schedulerName: kai-scheduler
  priorityClassName: build  # Non-preemptible interactive job
  containers:
  - name: interactive
    image: nvidia/cuda:11.8.0-base-ubuntu22.04
    resources:
      requests:
        nvidia.com/gpu: "4"
      limits:
        nvidia.com/gpu: "4"
    command: ["sleep", "3600"] 
