apiVersion: v1
kind: Pod
metadata:
  name: training-job-1
  labels:
    runai/queue: project-a
spec:
  schedulerName: kai-scheduler
  containers:
  - name: training
    image: gcr.io/run-ai-demo/quickstart-demo
    resources:
      requests:
        nvidia.com/gpu: "4"
      limits:
        nvidia.com/gpu: "4"
---
apiVersion: v1
kind: Pod
metadata:
  name: training-job-2
  labels:
    runai/queue: project-c
spec:
  schedulerName: kai-scheduler
  containers:
  - name: training
    image: gcr.io/run-ai-demo/quickstart-demo
    resources:
      requests:
        nvidia.com/gpu: "2"
      limits:
        nvidia.com/gpu: "2"
---
apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: pytorch-distributed-training
  labels:
    runai/queue: project-c
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          schedulerName: kai-scheduler
          containers:
            - args:
                - --backend
                - gloo
                - --epochs
                - "900"
                - --batch-size
                - "64"
                - --learning-rate
                - "0.01"
              image: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime
              name: pytorch
              command: ["python"]
              resources:
                requests:
                  nvidia.com/gpu: "2"
                limits:
                  nvidia.com/gpu: "2"
    Worker:
      replicas: 2
      restartPolicy: OnFailure
      template:
        spec:
          schedulerName: kai-scheduler
          containers:
            - args:
                - --backend
                - gloo
                - --epochs
                - "900"
                - --batch-size
                - "64"
                - --learning-rate
                - "0.01"
              image: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime
              name: pytorch
              command: ["python"]
              resources:
                requests:
                  nvidia.com/gpu: "2"
                limits:
                  nvidia.com/gpu: "2"
---
apiVersion: v1
kind: Pod
metadata:
  name: interactive-job
  labels:
    runai/queue: project-d
spec:
  schedulerName: kai-scheduler
  priorityClassName: build  # Non-preemptible interactive job
  containers:
  - name: interactive
    image: nvidia/cuda:11.8.0-base-ubuntu22.04
    resources:
      requests:
        nvidia.com/gpu: "5"
      limits:
        nvidia.com/gpu: "5"
    command: ["sleep", "3600"] 
