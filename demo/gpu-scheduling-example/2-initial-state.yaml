apiVersion: v1
kind: Pod
metadata:
  name: training-job-1
  labels:
    runai/queue: project-a
spec:
  schedulerName: kai-scheduler
  containers:
  - name: training
    image: gcr.io/run-ai-demo/quickstart-demo
    resources:
      requests:
        nvidia.com/gpu: "4"
      limits:
        nvidia.com/gpu: "4"
---
apiVersion: v1
kind: Pod
metadata:
  name: training-job-2
  labels:
    runai/queue: project-c
spec:
  schedulerName: kai-scheduler
  containers:
  - name: training
    image: gcr.io/run-ai-demo/quickstart-demo
    resources:
      requests:
        nvidia.com/gpu: "2"
      limits:
        nvidia.com/gpu: "2"
---
apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: pytorch-example
  labels:
    runai/queue: test
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          schedulerName: kai-scheduler
          containers:
            - args:
                - --backend
                - gloo
              image: gcr.io/kubeflow-ci/pytorch-dist-mnist-test:v1.0
              name: pytorch
              resources:
                limits:
                  nvidia.com/gpu: "1"
    Worker:
      replicas: 2
      restartPolicy: OnFailure
      template:
        spec:
          schedulerName: kai-scheduler
          containers:
            - args:
                - --backend
                - gloo
              image: gcr.io/kubeflow-ci/pytorch-dist-mnist-test:v1.0
              name: pytorch
              resources:
                limits:
                  nvidia.com/gpu: "1"
---
apiVersion: v1
kind: Pod
metadata:
  name: interactive-job
  labels:
    runai/queue: project-d
spec:
  schedulerName: kai-scheduler
  priorityClassName: build  # Non-preemptible interactive job
  containers:
  - name: interactive
    image: nvidia/cuda:11.8.0-base-ubuntu22.04
    resources:
      requests:
        nvidia.com/gpu: "5"
      limits:
        nvidia.com/gpu: "5"
    command: ["sleep", "3600"] 
